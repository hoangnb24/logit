{"id":"bd-13b","title":"Implement error taxonomy and exit code contract","description":"Guarantee meaningful failures and machine-usable exit statuses for automation and CI usage.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.531335Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.562077Z","compaction_level":0,"original_size":0,"labels":["cli","logit","reliability"],"dependencies":[{"issue_id":"bd-13b","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.532843Z","created_by":"themrb"}],"comments":[{"id":24,"issue_id":"bd-13b","author":"HoangNB","text":"Goal:\nGuarantee meaningful failures and machine-usable exit statuses for automation and CI usage.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-13y","title":"Epic: Test Matrix, Fixtures, and End-to-End Acceptance","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.544614Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.133770Z","compaction_level":0,"original_size":0,"labels":["epic","logit","test"],"dependencies":[{"issue_id":"bd-13y","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.545765Z","created_by":"themrb"}],"comments":[{"id":14,"issue_id":"bd-13y","author":"HoangNB","text":"Intent:\nEstablish high-confidence quality gates across unit, adapter, integration, and end-to-end layers.\n\nScope:\n- fixture corpus by agent\n- deterministic tests for mappings and edge cases\n- acceptance scenarios matching product goals\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-149","title":"Add snapshot integrity checks (counts, parseability, deterministic sampling)","description":"Ensure snapshot outputs are internally consistent and reproducible across runs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.197272Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.227722Z","compaction_level":0,"original_size":0,"labels":["logit","quality","snapshot"],"dependencies":[{"issue_id":"bd-149","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.198666Z","created_by":"themrb"}],"comments":[{"id":35,"issue_id":"bd-149","author":"HoangNB","text":"Goal:\nEnsure snapshot outputs are internally consistent and reproducible across runs.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-165","title":"Epic: Normalization Engine and agentlog.v1 Artifacts","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.451453Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.979782Z","compaction_level":0,"original_size":0,"labels":["epic","logit","normalize"],"dependencies":[{"issue_id":"bd-165","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.452537Z","created_by":"themrb"}],"comments":[{"id":11,"issue_id":"bd-165","author":"HoangNB","text":"Intent:\nBuild the core normalization pipeline and artifact emitter for agentlog.v1.\n\nScope:\n- canonical event struct + schema file\n- dedupe, ordering, timestamp normalization\n- JSONL writer and stats generation\n\nReasoning:\nThis is the semantic core of the project and all adapters converge here.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-182","title":"Implement runtime config resolution (paths, home expansion, outdir defaults)","description":"Resolve runtime paths deterministically, including default output location and override behaviors.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.467354Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.509529Z","compaction_level":0,"original_size":0,"labels":["cli","config","logit"],"dependencies":[{"issue_id":"bd-182","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.468683Z","created_by":"themrb"}],"comments":[{"id":23,"issue_id":"bd-182","author":"HoangNB","text":"Goal:\nResolve runtime paths deterministically, including default output location and override behaviors.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1d7","title":"Program: Build logit Rust CLI for multi-agent local log intelligence","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-25T06:55:51.578372Z","created_by":"themrb","updated_at":"2026-02-25T06:55:51.649061Z","compaction_level":0,"original_size":0,"labels":["logit","planning","program"],"comments":[{"id":1,"issue_id":"bd-1d7","author":"HoangNB","text":"Background:\nThis program turns ad-hoc, tool-specific local agent logs into a consistent, queryable operational dataset.\nWe are standardizing around a Rust CLI (`logit`) with deterministic output contracts and repeatable ingestion.\n\nWhy this exists:\n- We currently have heterogeneous local traces across Codex, Claude, Gemini, Amp, and OpenCode.\n- Valuable context is fragmented, difficult to inspect, and hard to compare across tools.\n- We need a durable normalization pipeline so future analysis, debugging, and automation are reliable.\n\nNorth-star outcomes:\n1) One canonical normalized event contract (agentlog.v1)\n2) Reproducible snapshot + normalize + validate workflow\n3) Optional SQLite mirror for local analytics\n4) Self-documenting execution and dependency graph in beads\n\nImplementation philosophy:\n- Non-destructive and read-only with respect to source logs\n- Deterministic transforms; minimal hidden behavior\n- Explicit acceptance criteria per work item\n- Strong observability (stats, reports, validation artifacts)\n\nProgram-level done criteria:\n- All child epics complete and integrated.\n- `logit normalize` generates valid schema + JSONL from all 5 adapters.\n- `logit validate` can certify outputs.\n- Evidence-based test matrix and docs are complete.\n","created_at":"2026-02-25T06:55:51Z"}]}
{"id":"bd-1dk","title":"Emit discovery artifacts (, )","description":"Persist discovery evidence so ingestion decisions are transparent and debuggable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.906140Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.948523Z","compaction_level":0,"original_size":0,"labels":["artifacts","discovery","logit"],"dependencies":[{"issue_id":"bd-1dk","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.907485Z","created_by":"themrb"}],"comments":[{"id":30,"issue_id":"bd-1dk","author":"HoangNB","text":"Goal:\nPersist discovery evidence so ingestion decisions are transparent and debuggable.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1gt","title":"Define privacy defaults: snapshot redaction rules vs normalize full-text policy","description":"Establish explicit data-handling defaults so operators know exactly what is redacted, retained, or transformed.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.180073Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.212653Z","compaction_level":0,"original_size":0,"labels":["architecture","logit","safety"],"dependencies":[{"issue_id":"bd-1gt","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.181461Z","created_by":"themrb"}],"comments":[{"id":18,"issue_id":"bd-1gt","author":"HoangNB","text":"Goal:\nEstablish explicit data-handling defaults so operators know exactly what is redacted, retained, or transformed.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1kq","title":"Epic: Optional SQLite Mirror","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.481796Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.025019Z","compaction_level":0,"original_size":0,"labels":["epic","logit","sqlite"],"dependencies":[{"issue_id":"bd-1kq","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.483029Z","created_by":"themrb"}],"comments":[{"id":12,"issue_id":"bd-1kq","author":"HoangNB","text":"Intent:\nProvide optional SQLite mirror for local analytics without changing primary JSONL contract.\n\nScope:\n- database schema aligned with canonical event model\n- efficient inserts and indexes\n- parity checks against emitted JSONL\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-1ly","title":"Implement text/content extraction helpers and derived  generation","description":"Standardize content extraction from diverse payload shapes while preserving full text where available.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.380957Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.413928Z","compaction_level":0,"original_size":0,"labels":["content","logit","normalize"],"dependencies":[{"issue_id":"bd-1ly","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.382777Z","created_by":"themrb"}],"comments":[{"id":38,"issue_id":"bd-1ly","author":"HoangNB","text":"Goal:\nStandardize content extraction from diverse payload shapes while preserving full text where available.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-1mo","title":"Define timestamp normalization hierarchy and ordering contract","description":"Specify how ISO/epoch variants are normalized, how null timestamps are handled, and how total order is produced.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.233922Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.271464Z","compaction_level":0,"original_size":0,"labels":["architecture","logit","time"],"dependencies":[{"issue_id":"bd-1mo","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.235438Z","created_by":"themrb"}],"comments":[{"id":19,"issue_id":"bd-1mo","author":"HoangNB","text":"Goal:\nSpecify how ISO/epoch variants are normalized, how null timestamps are handled, and how total order is produced.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1re","title":"Epic: Discovery and Source Inventory (including zsh history prioritization)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.207118Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.700585Z","compaction_level":0,"original_size":0,"labels":["discovery","epic","logit"],"dependencies":[{"issue_id":"bd-1re","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.208387Z","created_by":"themrb"}],"comments":[{"id":4,"issue_id":"bd-1re","author":"HoangNB","text":"Intent:\nImplement deterministic source discovery across all supported agents with transparent prioritization.\n\nScope:\n- well-known path discovery\n- source classification (jsonl/json/log/binary)\n- optional influence from shell history (`~/.zsh_history`)\n\nReasoning:\nDiscovery quality directly controls coverage and trust in downstream normalization.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-1sk","title":"Write snapshot artifacts per agent (, )","description":"Persist snapshot outputs in stable locations so they can be reviewed and versioned reliably.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.130996Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.177715Z","compaction_level":0,"original_size":0,"labels":["artifacts","logit","snapshot"],"dependencies":[{"issue_id":"bd-1sk","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.132645Z","created_by":"themrb"}],"comments":[{"id":34,"issue_id":"bd-1sk","author":"HoangNB","text":"Goal:\nPersist snapshot outputs in stable locations so they can be reviewed and versioned reliably.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-1w0","title":"Implement known-path discovery registry for all 5 supported agents","description":"Discover source roots across Codex, Claude, Gemini, Amp, and OpenCode with explicit path rules.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.649904Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.693066Z","compaction_level":0,"original_size":0,"labels":["discovery","logit","paths"],"dependencies":[{"issue_id":"bd-1w0","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.652269Z","created_by":"themrb"}],"comments":[{"id":26,"issue_id":"bd-1w0","author":"HoangNB","text":"Goal:\nDiscover source roots across Codex, Claude, Gemini, Amp, and OpenCode with explicit path rules.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-280","title":"Epic: Architecture, Contracts, and Safety Defaults","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.137994Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.618237Z","compaction_level":0,"original_size":0,"labels":["architecture","epic","logit"],"dependencies":[{"issue_id":"bd-280","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.139517Z","created_by":"themrb"}],"comments":[{"id":2,"issue_id":"bd-280","author":"HoangNB","text":"Intent:\nDefine irreversible project decisions early so downstream implementation is deterministic and low-friction.\n\nScope:\n- Canonical schema contract (agentlog.v1)\n- Output artifact topology and naming\n- Safety defaults (what is retained vs redacted in which mode)\n- Non-goals and compatibility boundaries\n\nWhy this matters:\nWithout stable contracts, adapter and pipeline work diverges and causes expensive rework.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-2lr","title":"Implement source classification (jsonl/json/text-log/binary)","description":"Classify discovered artifacts so downstream processors choose correct parsers and safety handling.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.714161Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.748467Z","compaction_level":0,"original_size":0,"labels":["classifier","discovery","logit"],"dependencies":[{"issue_id":"bd-2lr","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.715443Z","created_by":"themrb"}],"comments":[{"id":27,"issue_id":"bd-2lr","author":"HoangNB","text":"Goal:\nClassify discovered artifacts so downstream processors choose correct parsers and safety handling.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-2n2","title":"Specify run artifact topology and manifest contract under ","description":"Freeze directory layout, naming, and per-run metadata files so outputs are predictable and automation-friendly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.119063Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.159761Z","compaction_level":0,"original_size":0,"labels":["architecture","artifacts","logit"],"dependencies":[{"issue_id":"bd-2n2","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.120388Z","created_by":"themrb"}],"comments":[{"id":17,"issue_id":"bd-2n2","author":"HoangNB","text":"Goal:\nFreeze directory layout, naming, and per-run metadata files so outputs are predictable and automation-friendly.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-2uh","title":"Implement normalized JSONL writer, schema writer, and stats writer","description":"Emit canonical artifacts (, schema, stats) required for downstream consumption and QA.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.491941Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.525564Z","compaction_level":0,"original_size":0,"labels":["artifacts","logit","normalize"],"dependencies":[{"issue_id":"bd-2uh","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.493093Z","created_by":"themrb"}],"comments":[{"id":40,"issue_id":"bd-2uh","author":"HoangNB","text":"Goal:\nEmit canonical artifacts (, schema, stats) required for downstream consumption and QA.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-2x6","title":"Implement timestamp normalization utilities and canonical UTC conversion","description":"Normalize heterogeneous time formats into a single comparable representation with robust fallbacks.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.321924Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.359435Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","time"],"dependencies":[{"issue_id":"bd-2x6","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.323506Z","created_by":"themrb"}],"comments":[{"id":37,"issue_id":"bd-2x6","author":"HoangNB","text":"Goal:\nNormalize heterogeneous time formats into a single comparable representation with robust fallbacks.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-32g","title":"Implement  command baseline for source/normalized files","description":"Provide read-only introspection to quickly inspect inputs and outputs without custom scripts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.583645Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.628618Z","compaction_level":0,"original_size":0,"labels":["cli","inspect","logit"],"dependencies":[{"issue_id":"bd-32g","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.585214Z","created_by":"themrb"}],"comments":[{"id":25,"issue_id":"bd-32g","author":"HoangNB","text":"Goal:\nProvide read-only introspection to quickly inspect inputs and outputs without custom scripts.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-343","title":"Implement canonical Rust structs and schema generation for ","description":"Define the in-code data model and emit machine-readable schema artifact for validation and consumers.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.252525Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.294874Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","schema"],"dependencies":[{"issue_id":"bd-343","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.253992Z","created_by":"themrb"}],"comments":[{"id":36,"issue_id":"bd-343","author":"HoangNB","text":"Goal:\nDefine the in-code data model and emit machine-readable schema artifact for validation and consumers.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-386","title":"Scaffold Rust crate and module boundaries for ","description":"Create the foundational crate structure and module map to support adapter fan-in and command growth cleanly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.354964Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.385947Z","compaction_level":0,"original_size":0,"labels":["cli","logit","rust"],"dependencies":[{"issue_id":"bd-386","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.356220Z","created_by":"themrb"}],"comments":[{"id":21,"issue_id":"bd-386","author":"HoangNB","text":"Goal:\nCreate the foundational crate structure and module map to support adapter fan-in and command growth cleanly.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-38g","title":"Epic: Gemini Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.345572Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.859114Z","compaction_level":0,"original_size":0,"labels":["adapter","epic","gemini","logit"],"dependencies":[{"issue_id":"bd-38g","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.346960Z","created_by":"themrb"}],"comments":[{"id":8,"issue_id":"bd-38g","author":"HoangNB","text":"Intent:\nNormalize Gemini JSON sources and explicitly treat protobuf artifacts as indexed-only in v1.\n\nConsiderations:\n- logs.json may be sparse/empty\n- chats session files carry core message payloads\n- protobuf decoding intentionally deferred to avoid premature complexity\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-398","title":"Epic: Codex Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.277451Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.778328Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","epic","logit"],"dependencies":[{"issue_id":"bd-398","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.278898Z","created_by":"themrb"}],"comments":[{"id":6,"issue_id":"bd-398","author":"HoangNB","text":"Intent:\nNormalize Codex local artifacts into canonical events while preserving provenance.\n\nConsiderations:\n- rollout JSONL event types differ (response_item, event_msg, etc.)\n- history and tui logs provide auxiliary context\n- avoid duplicate inflation between history and rollout streams\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-39f","title":"Implement zsh history parser and per-agent command frequency scoring","description":"Use shell command frequency as a prioritization signal while keeping filesystem discovery authoritative.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.771416Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.807727Z","compaction_level":0,"original_size":0,"labels":["discovery","history","logit"],"dependencies":[{"issue_id":"bd-39f","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.772928Z","created_by":"themrb"}],"comments":[{"id":28,"issue_id":"bd-39f","author":"HoangNB","text":"Goal:\nUse shell command frequency as a prioritization signal while keeping filesystem discovery authoritative.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-39z","title":"Epic: Snapshot Pipeline (schema profile + representative samples)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.245712Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.740651Z","compaction_level":0,"original_size":0,"labels":["epic","logit","snapshot"],"dependencies":[{"issue_id":"bd-39z","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.247086Z","created_by":"themrb"}],"comments":[{"id":5,"issue_id":"bd-39z","author":"HoangNB","text":"Intent:\nProduce safe, representative source snapshots that are useful for debugging adapters and auditing ingestion quality.\n\nScope:\n- per-source schema/key profiling\n- event type frequency summaries\n- sample capture (default 3) with redaction safeguards\n\nReasoning:\nSnapshots are observability primitives for ingestion correctness.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-3ew","title":"Implement snapshot redaction and truncation pipeline","description":"Protect sensitive values in snapshot outputs while preserving enough context for troubleshooting.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.073770Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.104662Z","compaction_level":0,"original_size":0,"labels":["logit","safety","snapshot"],"dependencies":[{"issue_id":"bd-3ew","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.075035Z","created_by":"themrb"}],"comments":[{"id":33,"issue_id":"bd-3ew","author":"HoangNB","text":"Goal:\nProtect sensitive values in snapshot outputs while preserving enough context for troubleshooting.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-3hw","title":"Epic: Amp Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.385439Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.899605Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","epic","logit"],"dependencies":[{"issue_id":"bd-3hw","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.387010Z","created_by":"themrb"}],"comments":[{"id":9,"issue_id":"bd-3hw","author":"HoangNB","text":"Intent:\nNormalize Amp thread/message payloads and reconcile auxiliary file-change telemetry.\n\nConsiderations:\n- message content often nested as typed parts\n- file-change artifacts can be large and sensitive\n- keep transform deterministic and avoid over-parsing blobs\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-3i0","title":"Implement clap command surface (, , , )","description":"Expose stable user-facing commands and flags aligned with the project contract and acceptance flow.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.413980Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.445923Z","compaction_level":0,"original_size":0,"labels":["cli","commands","logit"],"dependencies":[{"issue_id":"bd-3i0","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.415433Z","created_by":"themrb"}],"comments":[{"id":22,"issue_id":"bd-3i0","author":"HoangNB","text":"Goal:\nExpose stable user-facing commands and flags aligned with the project contract and acceptance flow.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-3jm","title":"Define dedupe/provenance policy using raw hashes and fallback keys","description":"Prevent duplicate inflation while preserving traceability from normalized records back to source artifacts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.291500Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.333833Z","compaction_level":0,"original_size":0,"labels":["architecture","dedupe","logit"],"dependencies":[{"issue_id":"bd-3jm","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.292894Z","created_by":"themrb"}],"comments":[{"id":20,"issue_id":"bd-3jm","author":"HoangNB","text":"Goal:\nPrevent duplicate inflation while preserving traceability from normalized records back to source artifacts.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-3jn","title":"Implement representative sample extraction (default: 3 per source)","description":"Capture concise representative records for each source to aid debugging and adapter tuning.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.021734Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.054840Z","compaction_level":0,"original_size":0,"labels":["logit","samples","snapshot"],"dependencies":[{"issue_id":"bd-3jn","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.023344Z","created_by":"themrb"}],"comments":[{"id":32,"issue_id":"bd-3jn","author":"HoangNB","text":"Goal:\nCapture concise representative records for each source to aid debugging and adapter tuning.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-3ui","title":"Implement prioritization and filtering model (agent/source-kind/path)","description":"Allow deterministic ordering and targeted scans for focused runs and reproducibility.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.830009Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.878480Z","compaction_level":0,"original_size":0,"labels":["discovery","logit","priority"],"dependencies":[{"issue_id":"bd-3ui","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.831845Z","created_by":"themrb"}],"comments":[{"id":29,"issue_id":"bd-3ui","author":"HoangNB","text":"Goal:\nAllow deterministic ordering and targeted scans for focused runs and reproducibility.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-5k4","title":"Epic: Documentation, Operational Guidance, and Release Readiness","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.577684Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.169178Z","compaction_level":0,"original_size":0,"labels":["docs","epic","logit"],"dependencies":[{"issue_id":"bd-5k4","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.578911Z","created_by":"themrb"}],"comments":[{"id":15,"issue_id":"bd-5k4","author":"HoangNB","text":"Intent:\nMake the project operable by future contributors without oral context.\n\nScope:\n- architecture, command usage, safety notes\n- troubleshooting and known limitations\n- release checklist and acceptance evidence map\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-9eb","title":"Implement normalize orchestrator that fans in all adapters","description":"Coordinate adapter execution, normalization, and artifact emission in one deterministic command path.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.546197Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.598562Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","orchestration"],"dependencies":[{"issue_id":"bd-9eb","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.547469Z","created_by":"themrb"}],"comments":[{"id":41,"issue_id":"bd-9eb","author":"HoangNB","text":"Goal:\nCoordinate adapter execution, normalization, and artifact emission in one deterministic command path.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-9ej","title":"Implement dedupe engine and stable global event ordering","description":"Guarantee consistent event stream generation without duplicate inflation or nondeterministic ordering.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.434290Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.471216Z","compaction_level":0,"original_size":0,"labels":["dedupe","logit","normalize"],"dependencies":[{"issue_id":"bd-9ej","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.435655Z","created_by":"themrb"}],"comments":[{"id":39,"issue_id":"bd-9ej","author":"HoangNB","text":"Goal:\nGuarantee consistent event stream generation without duplicate inflation or nondeterministic ordering.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-jbe","title":"Implement per-source schema/key profiler and event-kind frequency analyzer","description":"Extract structural signatures from sources to monitor parser assumptions and drift.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.968676Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.000300Z","compaction_level":0,"original_size":0,"labels":["logit","profiling","snapshot"],"dependencies":[{"issue_id":"bd-jbe","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:38.970077Z","created_by":"themrb"}],"comments":[{"id":31,"issue_id":"bd-jbe","author":"HoangNB","text":"Goal:\nExtract structural signatures from sources to monitor parser assumptions and drift.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-lb0","title":"Epic: Claude Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.311322Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.820402Z","compaction_level":0,"original_size":0,"labels":["adapter","claude","epic","logit"],"dependencies":[{"issue_id":"bd-lb0","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.312933Z","created_by":"themrb"}],"comments":[{"id":7,"issue_id":"bd-lb0","author":"HoangNB","text":"Intent:\nNormalize Claude project/session artifacts including primary and subagent traces.\n\nConsiderations:\n- heterogeneous event kinds (`user`, `assistant`, `progress`, etc.)\n- mcp cache logs are diagnostic not conversational\n- maintain source tags for downstream filtering\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-nn2","title":"Define canonical  field semantics and invariants","description":"Codify the meaning, optionality, and allowed values of every normalized field so adapters and validators cannot diverge.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.051812Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.097598Z","compaction_level":0,"original_size":0,"labels":["architecture","contract","logit"],"dependencies":[{"issue_id":"bd-nn2","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.054375Z","created_by":"themrb"}],"comments":[{"id":16,"issue_id":"bd-nn2","author":"HoangNB","text":"Goal:\nCodify the meaning, optionality, and allowed values of every normalized field so adapters and validators cannot diverge.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-soe","title":"Epic: CLI Skeleton, Command Surface, and Runtime Plumbing","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.174448Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.658971Z","compaction_level":0,"original_size":0,"labels":["cli","epic","logit"],"dependencies":[{"issue_id":"bd-soe","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.175542Z","created_by":"themrb"}],"comments":[{"id":3,"issue_id":"bd-soe","author":"HoangNB","text":"Intent:\nCreate a robust Rust CLI shell that supports command growth without command-sprawl.\n\nScope:\n- `snapshot`, `normalize`, `inspect`, `validate` command family\n- global flags and predictable error model\n- filesystem-safe runtime behavior and clear exit codes\n\nDesign principle:\nThe command interface is part of the public API and must stay stable once published.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-u6d","title":"Epic: Validation and Consistency Reports","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.512433Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.087967Z","compaction_level":0,"original_size":0,"labels":["epic","logit","validate"],"dependencies":[{"issue_id":"bd-u6d","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.513645Z","created_by":"themrb"}],"comments":[{"id":13,"issue_id":"bd-u6d","author":"HoangNB","text":"Intent:\nShip a strict but practical validation layer that certifies normalized outputs.\n\nScope:\n- schema validation\n- invariant checks (required keys, timestamp sanity)\n- machine-readable validation report and exit codes\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-xdl","title":"Epic: OpenCode Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.420621Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.941936Z","compaction_level":0,"original_size":0,"labels":["adapter","epic","logit","opencode"],"dependencies":[{"issue_id":"bd-xdl","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.421920Z","created_by":"themrb"}],"comments":[{"id":10,"issue_id":"bd-xdl","author":"HoangNB","text":"Intent:\nNormalize OpenCode by joining session message metadata with session part payloads.\n\nConsiderations:\n- content frequently resides in part records, not message rows\n- require reliable join keys (`messageID`, `sessionID`)\n- preserve provider/model/cost metadata where available\n","created_at":"2026-02-25T06:56:29Z"}]}
