{"id":"bd-10h","title":"Implement SQLite writer with batched inserts and transaction handling","description":"Write normalized records to SQLite efficiently and safely using batched inserts and explicit transaction boundaries.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:56.738008Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.770626Z","compaction_level":0,"original_size":0,"labels":["logit","sqlite","writer"],"dependencies":[{"issue_id":"bd-10h","depends_on_id":"bd-1kq","type":"parent-child","created_at":"2026-02-25T06:58:56.739226Z","created_by":"themrb"}],"comments":[{"id":65,"issue_id":"bd-10h","author":"HoangNB","text":"Goal:\nWrite normalized records to SQLite efficiently and safely using batched inserts and explicit transaction boundaries.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-10m","title":"Parse Codex history.jsonl and merge as auxiliary prompts","description":"Ingest Codex history prompts as auxiliary events while avoiding duplicate amplification against rollout data.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.040640Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.076070Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","history","logit"],"dependencies":[{"issue_id":"bd-10m","depends_on_id":"bd-398","type":"parent-child","created_at":"2026-02-25T06:58:30.042131Z","created_by":"themrb"}],"comments":[{"id":43,"issue_id":"bd-10m","author":"HoangNB","text":"Goal:\nIngest Codex history prompts as auxiliary events while avoiding duplicate amplification against rollout data.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-12a","title":"Index Gemini protobuf conversation artifacts as snapshot-only metadata in v1","description":"Record protobuf conversation files for visibility without attempting protobuf decode in v1 normalization.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T06:58:30.552681Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.593566Z","compaction_level":0,"original_size":0,"labels":["adapter","gemini","logit","pb"],"dependencies":[{"issue_id":"bd-12a","depends_on_id":"bd-38g","type":"parent-child","created_at":"2026-02-25T06:58:30.554619Z","created_by":"themrb"}],"comments":[{"id":52,"issue_id":"bd-12a","author":"HoangNB","text":"Goal:\nRecord protobuf conversation files for visibility without attempting protobuf decode in v1 normalization.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-13b","title":"Implement error taxonomy and exit code contract","description":"Guarantee meaningful failures and machine-usable exit statuses for automation and CI usage.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.531335Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.562077Z","compaction_level":0,"original_size":0,"labels":["cli","logit","reliability"],"dependencies":[{"issue_id":"bd-13b","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.532843Z","created_by":"themrb"}],"comments":[{"id":24,"issue_id":"bd-13b","author":"HoangNB","text":"Goal:\nGuarantee meaningful failures and machine-usable exit statuses for automation and CI usage.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-13n","title":"Implement schema validation against generated agentlog.v1 schema","description":"Validate normalized JSONL records against the generated schema artifact and report line-level failures.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:56.915375Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.945499Z","compaction_level":0,"original_size":0,"labels":["logit","schema","validate"],"dependencies":[{"issue_id":"bd-13n","depends_on_id":"bd-u6d","type":"parent-child","created_at":"2026-02-25T06:58:56.916768Z","created_by":"themrb"}],"comments":[{"id":68,"issue_id":"bd-13n","author":"HoangNB","text":"Goal:\nValidate normalized JSONL records against the generated schema artifact and report line-level failures.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-13y","title":"Epic: Test Matrix, Fixtures, and End-to-End Acceptance","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.544614Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.133770Z","compaction_level":0,"original_size":0,"labels":["epic","logit","test"],"dependencies":[{"issue_id":"bd-13y","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.545765Z","created_by":"themrb"}],"comments":[{"id":14,"issue_id":"bd-13y","author":"HoangNB","text":"Intent:\nEstablish high-confidence quality gates across unit, adapter, integration, and end-to-end layers.\n\nScope:\n- fixture corpus by agent\n- deterministic tests for mappings and edge cases\n- acceptance scenarios matching product goals\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-149","title":"Add snapshot integrity checks (counts, parseability, deterministic sampling)","description":"Ensure snapshot outputs are internally consistent and reproducible across runs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.197272Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.227722Z","compaction_level":0,"original_size":0,"labels":["logit","quality","snapshot"],"dependencies":[{"issue_id":"bd-149","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.198666Z","created_by":"themrb"}],"comments":[{"id":35,"issue_id":"bd-149","author":"HoangNB","text":"Goal:\nEnsure snapshot outputs are internally consistent and reproducible across runs.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-165","title":"Epic: Normalization Engine and agentlog.v1 Artifacts","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.451453Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.979782Z","compaction_level":0,"original_size":0,"labels":["epic","logit","normalize"],"dependencies":[{"issue_id":"bd-165","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.452537Z","created_by":"themrb"}],"comments":[{"id":11,"issue_id":"bd-165","author":"HoangNB","text":"Intent:\nBuild the core normalization pipeline and artifact emitter for agentlog.v1.\n\nScope:\n- canonical event struct + schema file\n- dedupe, ordering, timestamp normalization\n- JSONL writer and stats generation\n\nReasoning:\nThis is the semantic core of the project and all adapters converge here.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-182","title":"Implement runtime config resolution (paths, home expansion, outdir defaults)","description":"Resolve runtime paths deterministically, including default output location and override behaviors.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.467354Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.509529Z","compaction_level":0,"original_size":0,"labels":["cli","config","logit"],"dependencies":[{"issue_id":"bd-182","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.468683Z","created_by":"themrb"}],"comments":[{"id":23,"issue_id":"bd-182","author":"HoangNB","text":"Goal:\nResolve runtime paths deterministically, including default output location and override behaviors.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1a1","title":"Amp adapter edge-case handling, blob limits, and mapping verification","description":"Consolidate Amp-specific size constraints, malformed artifact handling, and mapping verification cases.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.900517Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.931954Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","logit","quality"],"dependencies":[{"issue_id":"bd-1a1","depends_on_id":"bd-3hw","type":"parent-child","created_at":"2026-02-25T06:58:30.902233Z","created_by":"themrb"}],"comments":[{"id":58,"issue_id":"bd-1a1","author":"HoangNB","text":"Goal:\nConsolidate Amp-specific size constraints, malformed artifact handling, and mapping verification cases.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-1d7","title":"Program: Build logit Rust CLI for multi-agent local log intelligence","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-25T06:55:51.578372Z","created_by":"themrb","updated_at":"2026-02-25T06:55:51.649061Z","compaction_level":0,"original_size":0,"labels":["logit","planning","program"],"comments":[{"id":1,"issue_id":"bd-1d7","author":"HoangNB","text":"Background:\nThis program turns ad-hoc, tool-specific local agent logs into a consistent, queryable operational dataset.\nWe are standardizing around a Rust CLI (`logit`) with deterministic output contracts and repeatable ingestion.\n\nWhy this exists:\n- We currently have heterogeneous local traces across Codex, Claude, Gemini, Amp, and OpenCode.\n- Valuable context is fragmented, difficult to inspect, and hard to compare across tools.\n- We need a durable normalization pipeline so future analysis, debugging, and automation are reliable.\n\nNorth-star outcomes:\n1) One canonical normalized event contract (agentlog.v1)\n2) Reproducible snapshot + normalize + validate workflow\n3) Optional SQLite mirror for local analytics\n4) Self-documenting execution and dependency graph in beads\n\nImplementation philosophy:\n- Non-destructive and read-only with respect to source logs\n- Deterministic transforms; minimal hidden behavior\n- Explicit acceptance criteria per work item\n- Strong observability (stats, reports, validation artifacts)\n\nProgram-level done criteria:\n- All child epics complete and integrated.\n- `logit normalize` generates valid schema + JSONL from all 5 adapters.\n- `logit validate` can certify outputs.\n- Evidence-based test matrix and docs are complete.\n","created_at":"2026-02-25T06:55:51Z"}]}
{"id":"bd-1dk","title":"Emit discovery artifacts (sources.json, zsh_history_usage.json)","description":"Persist discovery evidence so ingestion decisions are transparent and debuggable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.906140Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.826186Z","compaction_level":0,"original_size":0,"labels":["artifacts","discovery","logit"],"dependencies":[{"issue_id":"bd-1dk","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.907485Z","created_by":"themrb"}],"comments":[{"id":30,"issue_id":"bd-1dk","author":"HoangNB","text":"Goal:\nPersist discovery evidence so ingestion decisions are transparent and debuggable.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1gt","title":"Define privacy defaults: snapshot redaction rules vs normalize full-text policy","description":"Establish explicit data-handling defaults so operators know exactly what is redacted, retained, or transformed.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.180073Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.212653Z","compaction_level":0,"original_size":0,"labels":["architecture","logit","safety"],"dependencies":[{"issue_id":"bd-1gt","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.181461Z","created_by":"themrb"}],"comments":[{"id":18,"issue_id":"bd-1gt","author":"HoangNB","text":"Goal:\nEstablish explicit data-handling defaults so operators know exactly what is redacted, retained, or transformed.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1k0","title":"Parse Gemini chat session JSON files and message arrays","description":"Map Gemini chat session message arrays into canonical events while preserving session context metadata.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.500062Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.530904Z","compaction_level":0,"original_size":0,"labels":["adapter","chats","gemini","logit"],"dependencies":[{"issue_id":"bd-1k0","depends_on_id":"bd-38g","type":"parent-child","created_at":"2026-02-25T06:58:30.501867Z","created_by":"themrb"}],"comments":[{"id":51,"issue_id":"bd-1k0","author":"HoangNB","text":"Goal:\nMap Gemini chat session message arrays into canonical events while preserving session context metadata.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-1kq","title":"Epic: Optional SQLite Mirror","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.481796Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.025019Z","compaction_level":0,"original_size":0,"labels":["epic","logit","sqlite"],"dependencies":[{"issue_id":"bd-1kq","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.483029Z","created_by":"themrb"}],"comments":[{"id":12,"issue_id":"bd-1kq","author":"HoangNB","text":"Intent:\nProvide optional SQLite mirror for local analytics without changing primary JSONL contract.\n\nScope:\n- database schema aligned with canonical event model\n- efficient inserts and indexes\n- parity checks against emitted JSONL\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-1ly","title":"Implement text and content extraction helpers plus derived excerpt generation","description":"Standardize content extraction from diverse payload shapes while preserving full text where available.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.380957Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.921799Z","compaction_level":0,"original_size":0,"labels":["content","logit","normalize"],"dependencies":[{"issue_id":"bd-1ly","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.382777Z","created_by":"themrb"}],"comments":[{"id":38,"issue_id":"bd-1ly","author":"HoangNB","text":"Goal:\nStandardize content extraction from diverse payload shapes while preserving full text where available.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-1mo","title":"Define timestamp normalization hierarchy and ordering contract","description":"Specify how ISO/epoch variants are normalized, how null timestamps are handled, and how total order is produced.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.233922Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.271464Z","compaction_level":0,"original_size":0,"labels":["architecture","logit","time"],"dependencies":[{"issue_id":"bd-1mo","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.235438Z","created_by":"themrb"}],"comments":[{"id":19,"issue_id":"bd-1mo","author":"HoangNB","text":"Goal:\nSpecify how ISO/epoch variants are normalized, how null timestamps are handled, and how total order is produced.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1re","title":"Epic: Discovery and Source Inventory (including zsh history prioritization)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.207118Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.700585Z","compaction_level":0,"original_size":0,"labels":["discovery","epic","logit"],"dependencies":[{"issue_id":"bd-1re","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.208387Z","created_by":"themrb"}],"comments":[{"id":4,"issue_id":"bd-1re","author":"HoangNB","text":"Intent:\nImplement deterministic source discovery across all supported agents with transparent prioritization.\n\nScope:\n- well-known path discovery\n- source classification (jsonl/json/log/binary)\n- optional influence from shell history (`~/.zsh_history`)\n\nReasoning:\nDiscovery quality directly controls coverage and trust in downstream normalization.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-1sk","title":"Write snapshot artifacts per agent (index.json, samples.jsonl)","description":"Persist snapshot outputs in stable locations so they can be reviewed and versioned reliably.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.130996Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.856684Z","compaction_level":0,"original_size":0,"labels":["artifacts","logit","snapshot"],"dependencies":[{"issue_id":"bd-1sk","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.132645Z","created_by":"themrb"}],"comments":[{"id":34,"issue_id":"bd-1sk","author":"HoangNB","text":"Goal:\nPersist snapshot outputs in stable locations so they can be reviewed and versioned reliably.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-1vv","title":"Parse Amp file-change artifacts and map tool/file telemetry safely","description":"Extract useful tool and file-change telemetry from Amp artifacts while truncating large before/after blobs safely.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.843353Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.880169Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","filechanges","logit"],"dependencies":[{"issue_id":"bd-1vv","depends_on_id":"bd-3hw","type":"parent-child","created_at":"2026-02-25T06:58:30.844855Z","created_by":"themrb"}],"comments":[{"id":57,"issue_id":"bd-1vv","author":"HoangNB","text":"Goal:\nExtract useful tool and file-change telemetry from Amp artifacts while truncating large before/after blobs safely.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-1w0","title":"Implement known-path discovery registry for all 5 supported agents","description":"Discover source roots across Codex, Claude, Gemini, Amp, and OpenCode with explicit path rules.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.649904Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.693066Z","compaction_level":0,"original_size":0,"labels":["discovery","logit","paths"],"dependencies":[{"issue_id":"bd-1w0","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.652269Z","created_by":"themrb"}],"comments":[{"id":26,"issue_id":"bd-1w0","author":"HoangNB","text":"Goal:\nDiscover source roots across Codex, Claude, Gemini, Amp, and OpenCode with explicit path rules.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-1xy","title":"Define SQLite mirror schema aligned to canonical normalized fields","description":"Create relational schema and table contracts that preserve all canonical normalized fields and provenance metadata.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:56.677684Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.716350Z","compaction_level":0,"original_size":0,"labels":["logit","schema","sqlite"],"dependencies":[{"issue_id":"bd-1xy","depends_on_id":"bd-1kq","type":"parent-child","created_at":"2026-02-25T06:58:56.680632Z","created_by":"themrb"}],"comments":[{"id":64,"issue_id":"bd-1xy","author":"HoangNB","text":"Goal:\nCreate relational schema and table contracts that preserve all canonical normalized fields and provenance metadata.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-20c","title":"Emit machine-readable validation report artifact","description":"Produce a structured validation report containing pass/fail, errors, warnings, and per-agent summary statistics.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.015316Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.048234Z","compaction_level":0,"original_size":0,"labels":["logit","report","validate"],"dependencies":[{"issue_id":"bd-20c","depends_on_id":"bd-u6d","type":"parent-child","created_at":"2026-02-25T06:58:57.016455Z","created_by":"themrb"}],"comments":[{"id":70,"issue_id":"bd-20c","author":"HoangNB","text":"Goal:\nProduce a structured validation report containing pass/fail, errors, warnings, and per-agent summary statistics.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-21o","title":"Define and enforce validation exit code contract","description":"Guarantee stable exit codes for success, validation failure, and runtime failure to support automation.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.069719Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.107963Z","compaction_level":0,"original_size":0,"labels":["cli","logit","validate"],"dependencies":[{"issue_id":"bd-21o","depends_on_id":"bd-u6d","type":"parent-child","created_at":"2026-02-25T06:58:57.071180Z","created_by":"themrb"}],"comments":[{"id":71,"issue_id":"bd-21o","author":"HoangNB","text":"Goal:\nGuarantee stable exit codes for success, validation failure, and runtime failure to support automation.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-241","title":"Implement integration tests for snapshot, normalize, and validate workflows","description":"Validate end-to-end command interactions and artifact generation across the primary workflow commands.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.312964Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.348118Z","compaction_level":0,"original_size":0,"labels":["integration","logit","test"],"dependencies":[{"issue_id":"bd-241","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.314330Z","created_by":"themrb"}],"comments":[{"id":75,"issue_id":"bd-241","author":"HoangNB","text":"Goal:\nValidate end-to-end command interactions and artifact generation across the primary workflow commands.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-24b","title":"Parse OpenCode session part records (content-bearing and step events)","description":"Ingest OpenCode part records, including text and non-text step events, with preserved provenance.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:31.009853Z","created_by":"themrb","updated_at":"2026-02-25T06:58:31.048129Z","compaction_level":0,"original_size":0,"labels":["adapter","logit","opencode","parts"],"dependencies":[{"issue_id":"bd-24b","depends_on_id":"bd-xdl","type":"parent-child","created_at":"2026-02-25T06:58:31.011248Z","created_by":"themrb"}],"comments":[{"id":60,"issue_id":"bd-24b","author":"HoangNB","text":"Goal:\nIngest OpenCode part records, including text and non-text step events, with preserved provenance.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:31Z"}]}
{"id":"bd-25d","title":"Parse Amp auxiliary history/session files","description":"Capture Amp history and session metadata as auxiliary context without duplicating thread message content.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.786107Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.823214Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","aux","logit"],"dependencies":[{"issue_id":"bd-25d","depends_on_id":"bd-3hw","type":"parent-child","created_at":"2026-02-25T06:58:30.788140Z","created_by":"themrb"}],"comments":[{"id":56,"issue_id":"bd-25d","author":"HoangNB","text":"Goal:\nCapture Amp history and session metadata as auxiliary context without duplicating thread message content.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-26k","title":"Implement invariant checks and strict-mode policy","description":"Add semantic checks beyond schema (required keys, timestamp sanity, null-rate heuristics) with optional strict mode.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:56.965916Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.995781Z","compaction_level":0,"original_size":0,"labels":["invariants","logit","validate"],"dependencies":[{"issue_id":"bd-26k","depends_on_id":"bd-u6d","type":"parent-child","created_at":"2026-02-25T06:58:56.967061Z","created_by":"themrb"}],"comments":[{"id":69,"issue_id":"bd-26k","author":"HoangNB","text":"Goal:\nAdd semantic checks beyond schema (required keys, timestamp sanity, null-rate heuristics) with optional strict mode.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-280","title":"Epic: Architecture, Contracts, and Safety Defaults","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.137994Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.618237Z","compaction_level":0,"original_size":0,"labels":["architecture","epic","logit"],"dependencies":[{"issue_id":"bd-280","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.139517Z","created_by":"themrb"}],"comments":[{"id":2,"issue_id":"bd-280","author":"HoangNB","text":"Intent:\nDefine irreversible project decisions early so downstream implementation is deterministic and low-friction.\n\nScope:\n- Canonical schema contract (agentlog.v1)\n- Output artifact topology and naming\n- Safety defaults (what is retained vs redacted in which mode)\n- Non-goals and compatibility boundaries\n\nWhy this matters:\nWithout stable contracts, adapter and pipeline work diverges and causes expensive rework.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-28v","title":"Parse OpenCode session info and message metadata records","description":"Ingest OpenCode session info and message metadata rows as the base layer for later content joins.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.954486Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.989734Z","compaction_level":0,"original_size":0,"labels":["adapter","logit","message-meta","opencode"],"dependencies":[{"issue_id":"bd-28v","depends_on_id":"bd-xdl","type":"parent-child","created_at":"2026-02-25T06:58:30.956223Z","created_by":"themrb"}],"comments":[{"id":59,"issue_id":"bd-28v","author":"HoangNB","text":"Goal:\nIngest OpenCode session info and message metadata rows as the base layer for later content joins.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-29y","title":"Build fixture corpus layout for all supported agent source shapes","description":"Establish a reusable fixture corpus that captures representative and edge-case source payloads for every adapter.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.130242Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.166091Z","compaction_level":0,"original_size":0,"labels":["fixtures","logit","test"],"dependencies":[{"issue_id":"bd-29y","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.131668Z","created_by":"themrb"}],"comments":[{"id":72,"issue_id":"bd-29y","author":"HoangNB","text":"Goal:\nEstablish a reusable fixture corpus that captures representative and edge-case source payloads for every adapter.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-29z","title":"Implement SQLite indexes and query sanity checks","description":"Add practical indexes and basic query sanity checks to ensure local analytics performance and correctness.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:56.791356Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.835393Z","compaction_level":0,"original_size":0,"labels":["indexing","logit","sqlite"],"dependencies":[{"issue_id":"bd-29z","depends_on_id":"bd-1kq","type":"parent-child","created_at":"2026-02-25T06:58:56.792753Z","created_by":"themrb"}],"comments":[{"id":66,"issue_id":"bd-29z","author":"HoangNB","text":"Goal:\nAdd practical indexes and basic query sanity checks to ensure local analytics performance and correctness.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-2il","title":"Implement adapter-level mapping tests for all five adapters","description":"Verify each adapter mapping contract and edge-case handling against fixture inputs and expected canonical outputs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.249173Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.291515Z","compaction_level":0,"original_size":0,"labels":["adapters","logit","test"],"dependencies":[{"issue_id":"bd-2il","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.250715Z","created_by":"themrb"}],"comments":[{"id":74,"issue_id":"bd-2il","author":"HoangNB","text":"Goal:\nVerify each adapter mapping contract and edge-case handling against fixture inputs and expected canonical outputs.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-2j8","title":"Parse Codex rollout JSONL primary event stream","description":"Map Codex rollout JSONL records (including response_item and event_msg families) into canonical events with provenance.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:29.974193Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.015667Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","logit","rollout"],"dependencies":[{"issue_id":"bd-2j8","depends_on_id":"bd-398","type":"parent-child","created_at":"2026-02-25T06:58:29.976829Z","created_by":"themrb"}],"comments":[{"id":42,"issue_id":"bd-2j8","author":"HoangNB","text":"Goal:\nMap Codex rollout JSONL records (including response_item and event_msg families) into canonical events with provenance.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-2lr","title":"Implement source classification (jsonl/json/text-log/binary)","description":"Classify discovered artifacts so downstream processors choose correct parsers and safety handling.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.714161Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.748467Z","compaction_level":0,"original_size":0,"labels":["classifier","discovery","logit"],"dependencies":[{"issue_id":"bd-2lr","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.715443Z","created_by":"themrb"}],"comments":[{"id":27,"issue_id":"bd-2lr","author":"HoangNB","text":"Goal:\nClassify discovered artifacts so downstream processors choose correct parsers and safety handling.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-2n2","title":"Specify run artifact topology and manifest contract under ~/.logit/output","description":"Freeze directory layout, naming, and per-run metadata files so outputs are predictable and automation-friendly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.119063Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.696750Z","compaction_level":0,"original_size":0,"labels":["architecture","artifacts","logit"],"dependencies":[{"issue_id":"bd-2n2","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.120388Z","created_by":"themrb"}],"comments":[{"id":17,"issue_id":"bd-2n2","author":"HoangNB","text":"Goal:\nFreeze directory layout, naming, and per-run metadata files so outputs are predictable and automation-friendly.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-2pj","title":"Parse Amp thread JSON envelopes and thread-level metadata","description":"Ingest Amp thread containers and extract stable thread/session metadata required for canonical event context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.671224Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.702094Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","logit","threads"],"dependencies":[{"issue_id":"bd-2pj","depends_on_id":"bd-3hw","type":"parent-child","created_at":"2026-02-25T06:58:30.672684Z","created_by":"themrb"}],"comments":[{"id":54,"issue_id":"bd-2pj","author":"HoangNB","text":"Goal:\nIngest Amp thread containers and extract stable thread/session metadata required for canonical event context.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-2tt","title":"Write privacy, safety, and known-limitations documentation","description":"Document data handling defaults, redaction behavior, and intentional v1 limitations for operator clarity.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.614843Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.658025Z","compaction_level":0,"original_size":0,"labels":["docs","logit","safety"],"dependencies":[{"issue_id":"bd-2tt","depends_on_id":"bd-5k4","type":"parent-child","created_at":"2026-02-25T06:58:57.616188Z","created_by":"themrb"}],"comments":[{"id":80,"issue_id":"bd-2tt","author":"HoangNB","text":"Goal:\nDocument data handling defaults, redaction behavior, and intentional v1 limitations for operator clarity.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-2uh","title":"Implement normalized JSONL writer, schema writer, and stats writer","description":"Emit canonical artifacts (events.jsonl, schema, stats) required for downstream consumption and QA.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.491941Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.951371Z","compaction_level":0,"original_size":0,"labels":["artifacts","logit","normalize"],"dependencies":[{"issue_id":"bd-2uh","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.493093Z","created_by":"themrb"}],"comments":[{"id":40,"issue_id":"bd-2uh","author":"HoangNB","text":"Goal:\nEmit canonical artifacts (, schema, stats) required for downstream consumption and QA.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-2vc","title":"Parse Claude project session JSONL event records","description":"Map primary Claude project session records (user, assistant, progress, system) into canonical events with preserved roles.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.214737Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.253682Z","compaction_level":0,"original_size":0,"labels":["adapter","claude","logit","session"],"dependencies":[{"issue_id":"bd-2vc","depends_on_id":"bd-lb0","type":"parent-child","created_at":"2026-02-25T06:58:30.216415Z","created_by":"themrb"}],"comments":[{"id":46,"issue_id":"bd-2vc","author":"HoangNB","text":"Goal:\nMap primary Claude project session records (user, assistant, progress, system) into canonical events with preserved roles.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-2x6","title":"Implement timestamp normalization utilities and canonical UTC conversion","description":"Normalize heterogeneous time formats into a single comparable representation with robust fallbacks.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.321924Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.359435Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","time"],"dependencies":[{"issue_id":"bd-2x6","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.323506Z","created_by":"themrb"}],"comments":[{"id":37,"issue_id":"bd-2x6","author":"HoangNB","text":"Goal:\nNormalize heterogeneous time formats into a single comparable representation with robust fallbacks.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-2yx","title":"OpenCode adapter edge-case handling and mapping verification","description":"Consolidate OpenCode join, nullability, and part-type variance into explicit mapping verification cases.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:31.185083Z","created_by":"themrb","updated_at":"2026-02-25T06:58:31.219798Z","compaction_level":0,"original_size":0,"labels":["adapter","logit","opencode","quality"],"dependencies":[{"issue_id":"bd-2yx","depends_on_id":"bd-xdl","type":"parent-child","created_at":"2026-02-25T06:58:31.187064Z","created_by":"themrb"}],"comments":[{"id":63,"issue_id":"bd-2yx","author":"HoangNB","text":"Goal:\nConsolidate OpenCode join, nullability, and part-type variance into explicit mapping verification cases.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:31Z"}]}
{"id":"bd-32e","title":"Implement OpenCode message-to-part join strategy with graceful failure behavior","description":"Join OpenCode part records to message metadata via IDs and define explicit behavior for missing join targets.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:31.069171Z","created_by":"themrb","updated_at":"2026-02-25T06:58:31.102322Z","compaction_level":0,"original_size":0,"labels":["adapter","join","logit","opencode"],"dependencies":[{"issue_id":"bd-32e","depends_on_id":"bd-xdl","type":"parent-child","created_at":"2026-02-25T06:58:31.070886Z","created_by":"themrb"}],"comments":[{"id":61,"issue_id":"bd-32e","author":"HoangNB","text":"Goal:\nJoin OpenCode part records to message metadata via IDs and define explicit behavior for missing join targets.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:31Z"}]}
{"id":"bd-32g","title":"Implement inspect command baseline for source and normalized files","description":"Provide read-only introspection to quickly inspect inputs and outputs without custom scripts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.583645Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.791892Z","compaction_level":0,"original_size":0,"labels":["cli","inspect","logit"],"dependencies":[{"issue_id":"bd-32g","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.585214Z","created_by":"themrb"}],"comments":[{"id":25,"issue_id":"bd-32g","author":"HoangNB","text":"Goal:\nProvide read-only introspection to quickly inspect inputs and outputs without custom scripts.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-343","title":"Implement canonical Rust structs and schema generation for agentlog.v1","description":"Define the in-code data model and emit machine-readable schema artifact for validation and consumers.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.252525Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.895919Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","schema"],"dependencies":[{"issue_id":"bd-343","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.253992Z","created_by":"themrb"}],"comments":[{"id":36,"issue_id":"bd-343","author":"HoangNB","text":"Goal:\nDefine the in-code data model and emit machine-readable schema artifact for validation and consumers.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-34x","title":"Extract Amp typed message content parts into canonical text and parts","description":"Normalize Amp nested typed content arrays into full text, excerpts, and structured content_parts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:30.723235Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.766681Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","content","logit"],"dependencies":[{"issue_id":"bd-34x","depends_on_id":"bd-3hw","type":"parent-child","created_at":"2026-02-25T06:58:30.725059Z","created_by":"themrb"}],"comments":[{"id":55,"issue_id":"bd-34x","author":"HoangNB","text":"Goal:\nNormalize Amp nested typed content arrays into full text, excerpts, and structured content_parts.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-386","title":"Scaffold Rust crate and module boundaries for logit","description":"Create the foundational crate structure and module map to support adapter fan-in and command growth cleanly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.354964Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.727589Z","compaction_level":0,"original_size":0,"labels":["cli","logit","rust"],"dependencies":[{"issue_id":"bd-386","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.356220Z","created_by":"themrb"}],"comments":[{"id":21,"issue_id":"bd-386","author":"HoangNB","text":"Goal:\nCreate the foundational crate structure and module map to support adapter fan-in and command growth cleanly.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-38g","title":"Epic: Gemini Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.345572Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.859114Z","compaction_level":0,"original_size":0,"labels":["adapter","epic","gemini","logit"],"dependencies":[{"issue_id":"bd-38g","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.346960Z","created_by":"themrb"}],"comments":[{"id":8,"issue_id":"bd-38g","author":"HoangNB","text":"Intent:\nNormalize Gemini JSON sources and explicitly treat protobuf artifacts as indexed-only in v1.\n\nConsiderations:\n- logs.json may be sparse/empty\n- chats session files carry core message payloads\n- protobuf decoding intentionally deferred to avoid premature complexity\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-398","title":"Epic: Codex Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.277451Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.778328Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","epic","logit"],"dependencies":[{"issue_id":"bd-398","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.278898Z","created_by":"themrb"}],"comments":[{"id":6,"issue_id":"bd-398","author":"HoangNB","text":"Intent:\nNormalize Codex local artifacts into canonical events while preserving provenance.\n\nConsiderations:\n- rollout JSONL event types differ (response_item, event_msg, etc.)\n- history and tui logs provide auxiliary context\n- avoid duplicate inflation between history and rollout streams\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-39f","title":"Implement zsh history parser and per-agent command frequency scoring","description":"Use shell command frequency as a prioritization signal while keeping filesystem discovery authoritative.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.771416Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.807727Z","compaction_level":0,"original_size":0,"labels":["discovery","history","logit"],"dependencies":[{"issue_id":"bd-39f","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.772928Z","created_by":"themrb"}],"comments":[{"id":28,"issue_id":"bd-39f","author":"HoangNB","text":"Goal:\nUse shell command frequency as a prioritization signal while keeping filesystem discovery authoritative.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-39z","title":"Epic: Snapshot Pipeline (schema profile + representative samples)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.245712Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.740651Z","compaction_level":0,"original_size":0,"labels":["epic","logit","snapshot"],"dependencies":[{"issue_id":"bd-39z","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.247086Z","created_by":"themrb"}],"comments":[{"id":5,"issue_id":"bd-39z","author":"HoangNB","text":"Intent:\nProduce safe, representative source snapshots that are useful for debugging adapters and auditing ingestion quality.\n\nScope:\n- per-source schema/key profiling\n- event type frequency summaries\n- sample capture (default 3) with redaction safeguards\n\nReasoning:\nSnapshots are observability primitives for ingestion correctness.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-3ck","title":"Configure CI test matrix and deterministic execution settings","description":"Define CI execution strategy, deterministic settings, and quality gates for ongoing reliability.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.426602Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.461887Z","compaction_level":0,"original_size":0,"labels":["ci","logit","test"],"dependencies":[{"issue_id":"bd-3ck","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.428237Z","created_by":"themrb"}],"comments":[{"id":77,"issue_id":"bd-3ck","author":"HoangNB","text":"Goal:\nDefine CI execution strategy, deterministic settings, and quality gates for ongoing reliability.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-3ew","title":"Implement snapshot redaction and truncation pipeline","description":"Protect sensitive values in snapshot outputs while preserving enough context for troubleshooting.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.073770Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.104662Z","compaction_level":0,"original_size":0,"labels":["logit","safety","snapshot"],"dependencies":[{"issue_id":"bd-3ew","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.075035Z","created_by":"themrb"}],"comments":[{"id":33,"issue_id":"bd-3ew","author":"HoangNB","text":"Goal:\nProtect sensitive values in snapshot outputs while preserving enough context for troubleshooting.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-3hw","title":"Epic: Amp Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.385439Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.899605Z","compaction_level":0,"original_size":0,"labels":["adapter","amp","epic","logit"],"dependencies":[{"issue_id":"bd-3hw","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.387010Z","created_by":"themrb"}],"comments":[{"id":9,"issue_id":"bd-3hw","author":"HoangNB","text":"Intent:\nNormalize Amp thread/message payloads and reconcile auxiliary file-change telemetry.\n\nConsiderations:\n- message content often nested as typed parts\n- file-change artifacts can be large and sensitive\n- keep transform deterministic and avoid over-parsing blobs\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-3i0","title":"Implement clap command surface (snapshot, normalize, inspect, validate)","description":"Expose stable user-facing commands and flags aligned with the project contract and acceptance flow.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.413980Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.763386Z","compaction_level":0,"original_size":0,"labels":["cli","commands","logit"],"dependencies":[{"issue_id":"bd-3i0","depends_on_id":"bd-soe","type":"parent-child","created_at":"2026-02-25T06:57:38.415433Z","created_by":"themrb"}],"comments":[{"id":22,"issue_id":"bd-3i0","author":"HoangNB","text":"Goal:\nExpose stable user-facing commands and flags aligned with the project contract and acceptance flow.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-3jm","title":"Define dedupe/provenance policy using raw hashes and fallback keys","description":"Prevent duplicate inflation while preserving traceability from normalized records back to source artifacts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.291500Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.333833Z","compaction_level":0,"original_size":0,"labels":["architecture","dedupe","logit"],"dependencies":[{"issue_id":"bd-3jm","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.292894Z","created_by":"themrb"}],"comments":[{"id":20,"issue_id":"bd-3jm","author":"HoangNB","text":"Goal:\nPrevent duplicate inflation while preserving traceability from normalized records back to source artifacts.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-3jn","title":"Implement representative sample extraction (default: 3 per source)","description":"Capture concise representative records for each source to aid debugging and adapter tuning.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:39.021734Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.054840Z","compaction_level":0,"original_size":0,"labels":["logit","samples","snapshot"],"dependencies":[{"issue_id":"bd-3jn","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:39.023344Z","created_by":"themrb"}],"comments":[{"id":32,"issue_id":"bd-3jn","author":"HoangNB","text":"Goal:\nCapture concise representative records for each source to aid debugging and adapter tuning.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-3kf","title":"Write troubleshooting and failure-mode cookbook","description":"Provide practical guidance for diagnosing parse failures, validation errors, and discovery gaps.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.681509Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.723087Z","compaction_level":0,"original_size":0,"labels":["docs","logit","operations"],"dependencies":[{"issue_id":"bd-3kf","depends_on_id":"bd-5k4","type":"parent-child","created_at":"2026-02-25T06:58:57.682922Z","created_by":"themrb"}],"comments":[{"id":81,"issue_id":"bd-3kf","author":"HoangNB","text":"Goal:\nProvide practical guidance for diagnosing parse failures, validation errors, and discovery gaps.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-3t3","title":"Implement utility unit tests (time, hash, redaction, history parsing)","description":"Add deterministic unit tests for shared utility modules that underpin normalization correctness.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.186437Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.227632Z","compaction_level":0,"original_size":0,"labels":["logit","test","unit"],"dependencies":[{"issue_id":"bd-3t3","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.187975Z","created_by":"themrb"}],"comments":[{"id":73,"issue_id":"bd-3t3","author":"HoangNB","text":"Goal:\nAdd deterministic unit tests for shared utility modules that underpin normalization correctness.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-3ui","title":"Implement prioritization and filtering model (agent/source-kind/path)","description":"Allow deterministic ordering and targeted scans for focused runs and reproducibility.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:57:38.830009Z","created_by":"themrb","updated_at":"2026-02-25T06:57:38.878480Z","compaction_level":0,"original_size":0,"labels":["discovery","logit","priority"],"dependencies":[{"issue_id":"bd-3ui","depends_on_id":"bd-1re","type":"parent-child","created_at":"2026-02-25T06:57:38.831845Z","created_by":"themrb"}],"comments":[{"id":29,"issue_id":"bd-3ui","author":"HoangNB","text":"Goal:\nAllow deterministic ordering and targeted scans for focused runs and reproducibility.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-3uo","title":"Claude adapter edge-case handling and mapping verification","description":"Consolidate Claude-specific event-shape variance and verify stable mapping across mixed record types.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.382730Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.415211Z","compaction_level":0,"original_size":0,"labels":["adapter","claude","logit","quality"],"dependencies":[{"issue_id":"bd-3uo","depends_on_id":"bd-lb0","type":"parent-child","created_at":"2026-02-25T06:58:30.384443Z","created_by":"themrb"}],"comments":[{"id":49,"issue_id":"bd-3uo","author":"HoangNB","text":"Goal:\nConsolidate Claude-specific event-shape variance and verify stable mapping across mixed record types.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-4t2","title":"Gemini adapter edge-case handling and mapping verification","description":"Consolidate Gemini nullability and sparse payload behaviors into explicit mapping and test expectations.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.614014Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.651010Z","compaction_level":0,"original_size":0,"labels":["adapter","gemini","logit","quality"],"dependencies":[{"issue_id":"bd-4t2","depends_on_id":"bd-38g","type":"parent-child","created_at":"2026-02-25T06:58:30.616069Z","created_by":"themrb"}],"comments":[{"id":53,"issue_id":"bd-4t2","author":"HoangNB","text":"Goal:\nConsolidate Gemini nullability and sparse payload behaviors into explicit mapping and test expectations.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-5k4","title":"Epic: Documentation, Operational Guidance, and Release Readiness","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.577684Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.169178Z","compaction_level":0,"original_size":0,"labels":["docs","epic","logit"],"dependencies":[{"issue_id":"bd-5k4","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.578911Z","created_by":"themrb"}],"comments":[{"id":15,"issue_id":"bd-5k4","author":"HoangNB","text":"Intent:\nMake the project operable by future contributors without oral context.\n\nScope:\n- architecture, command usage, safety notes\n- troubleshooting and known limitations\n- release checklist and acceptance evidence map\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-8o6","title":"Write release checklist and acceptance evidence template","description":"Define a repeatable release readiness checklist and evidence template tied to acceptance criteria.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.744217Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.778823Z","compaction_level":0,"original_size":0,"labels":["docs","logit","release"],"dependencies":[{"issue_id":"bd-8o6","depends_on_id":"bd-5k4","type":"parent-child","created_at":"2026-02-25T06:58:57.745709Z","created_by":"themrb"}],"comments":[{"id":82,"issue_id":"bd-8o6","author":"HoangNB","text":"Goal:\nDefine a repeatable release readiness checklist and evidence template tied to acceptance criteria.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-9eb","title":"Implement normalize orchestrator that fans in all adapters","description":"Coordinate adapter execution, normalization, and artifact emission in one deterministic command path.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.546197Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.598562Z","compaction_level":0,"original_size":0,"labels":["logit","normalize","orchestration"],"dependencies":[{"issue_id":"bd-9eb","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.547469Z","created_by":"themrb"}],"comments":[{"id":41,"issue_id":"bd-9eb","author":"HoangNB","text":"Goal:\nCoordinate adapter execution, normalization, and artifact emission in one deterministic command path.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-9ej","title":"Implement dedupe engine and stable global event ordering","description":"Guarantee consistent event stream generation without duplicate inflation or nondeterministic ordering.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:39.434290Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.471216Z","compaction_level":0,"original_size":0,"labels":["dedupe","logit","normalize"],"dependencies":[{"issue_id":"bd-9ej","depends_on_id":"bd-165","type":"parent-child","created_at":"2026-02-25T06:57:39.435655Z","created_by":"themrb"}],"comments":[{"id":39,"issue_id":"bd-9ej","author":"HoangNB","text":"Goal:\nGuarantee consistent event stream generation without duplicate inflation or nondeterministic ordering.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-9rt","title":"Implement local environment smoke tests for discovery and adapter coverage","description":"Add smoke checks that ensure discovery and normalization can run on realistic local directory structures.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:57.369851Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.404954Z","compaction_level":0,"original_size":0,"labels":["logit","smoke","test"],"dependencies":[{"issue_id":"bd-9rt","depends_on_id":"bd-13y","type":"parent-child","created_at":"2026-02-25T06:58:57.371286Z","created_by":"themrb"}],"comments":[{"id":76,"issue_id":"bd-9rt","author":"HoangNB","text":"Goal:\nAdd smoke checks that ensure discovery and normalization can run on realistic local directory structures.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-9yb","title":"Parse Gemini tmp logs.json arrays (including empty-array cases)","description":"Handle Gemini logs.json array payloads robustly, including empty arrays and sparse message entries.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.439706Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.479560Z","compaction_level":0,"original_size":0,"labels":["adapter","gemini","logit","logs"],"dependencies":[{"issue_id":"bd-9yb","depends_on_id":"bd-38g","type":"parent-child","created_at":"2026-02-25T06:58:30.441605Z","created_by":"themrb"}],"comments":[{"id":50,"issue_id":"bd-9yb","author":"HoangNB","text":"Goal:\nHandle Gemini logs.json array payloads robustly, including empty arrays and sparse message entries.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-fnj","title":"Parse Claude subagent session traces with explicit tagging","description":"Ingest Claude subagent traces and tag them so downstream analysis can separate primary and delegated activity.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.273251Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.306777Z","compaction_level":0,"original_size":0,"labels":["adapter","claude","logit","subagent"],"dependencies":[{"issue_id":"bd-fnj","depends_on_id":"bd-lb0","type":"parent-child","created_at":"2026-02-25T06:58:30.274739Z","created_by":"themrb"}],"comments":[{"id":47,"issue_id":"bd-fnj","author":"HoangNB","text":"Goal:\nIngest Claude subagent traces and tag them so downstream analysis can separate primary and delegated activity.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-fqr","title":"Codex adapter edge-case handling and mapping verification","description":"Consolidate Codex-specific null handling, unknown event-kind fallbacks, and verification cases.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.158488Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.192729Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","logit","quality"],"dependencies":[{"issue_id":"bd-fqr","depends_on_id":"bd-398","type":"parent-child","created_at":"2026-02-25T06:58:30.160302Z","created_by":"themrb"}],"comments":[{"id":45,"issue_id":"bd-fqr","author":"HoangNB","text":"Goal:\nConsolidate Codex-specific null handling, unknown event-kind fallbacks, and verification cases.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-jbe","title":"Implement per-source schema/key profiler and event-kind frequency analyzer","description":"Extract structural signatures from sources to monitor parser assumptions and drift.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.968676Z","created_by":"themrb","updated_at":"2026-02-25T06:57:39.000300Z","compaction_level":0,"original_size":0,"labels":["logit","profiling","snapshot"],"dependencies":[{"issue_id":"bd-jbe","depends_on_id":"bd-39z","type":"parent-child","created_at":"2026-02-25T06:57:38.970077Z","created_by":"themrb"}],"comments":[{"id":31,"issue_id":"bd-jbe","author":"HoangNB","text":"Goal:\nExtract structural signatures from sources to monitor parser assumptions and drift.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:39Z"}]}
{"id":"bd-l8n","title":"Parse Claude history and MCP cache logs as auxiliary sources","description":"Capture Claude history plus mcp cache debug logs with clear source-kind labels and non-conversational typing where appropriate.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:30.327647Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.363312Z","compaction_level":0,"original_size":0,"labels":["adapter","aux","claude","logit"],"dependencies":[{"issue_id":"bd-l8n","depends_on_id":"bd-lb0","type":"parent-child","created_at":"2026-02-25T06:58:30.329341Z","created_by":"themrb"}],"comments":[{"id":48,"issue_id":"bd-l8n","author":"HoangNB","text":"Goal:\nCapture Claude history plus mcp cache debug logs with clear source-kind labels and non-conversational typing where appropriate.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-lb0","title":"Epic: Claude Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.311322Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.820402Z","compaction_level":0,"original_size":0,"labels":["adapter","claude","epic","logit"],"dependencies":[{"issue_id":"bd-lb0","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.312933Z","created_by":"themrb"}],"comments":[{"id":7,"issue_id":"bd-lb0","author":"HoangNB","text":"Intent:\nNormalize Claude project/session artifacts including primary and subagent traces.\n\nConsiderations:\n- heterogeneous event kinds (`user`, `assistant`, `progress`, etc.)\n- mcp cache logs are diagnostic not conversational\n- maintain source tags for downstream filtering\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-nn2","title":"Define canonical agentlog.v1 field semantics and invariants","description":"Codify the meaning, optionality, and allowed values of every normalized field so adapters and validators cannot diverge.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:57:38.051812Z","created_by":"themrb","updated_at":"2026-02-25T06:58:29.660239Z","compaction_level":0,"original_size":0,"labels":["architecture","contract","logit"],"dependencies":[{"issue_id":"bd-nn2","depends_on_id":"bd-280","type":"parent-child","created_at":"2026-02-25T06:57:38.054375Z","created_by":"themrb"}],"comments":[{"id":16,"issue_id":"bd-nn2","author":"HoangNB","text":"Goal:\nCodify the meaning, optionality, and allowed values of every normalized field so adapters and validators cannot diverge.\n\nBackground / reasoning:\nThis task exists to make the  pipeline deterministic, auditable, and maintainable over time.\nThe deliverable should avoid hidden assumptions and make behavior explicit for future contributors.\n\nImplementation notes:\n- Prefer pure functions and explicit data contracts.\n- Preserve provenance metadata to support debugging.\n- Keep output stable to avoid flaky downstream tooling.\n\nDefinition of done:\n- Behavior is implemented and testable.\n- Edge cases relevant to this scope are handled or explicitly documented.\n- Task output is consumable by dependent tasks without reinterpretation.\n","created_at":"2026-02-25T06:57:38Z"}]}
{"id":"bd-ohq","title":"Implement parity checks between JSONL and SQLite mirror outputs","description":"Verify mirror integrity by comparing record counts and critical fields between JSONL and SQLite outputs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:56.854537Z","created_by":"themrb","updated_at":"2026-02-25T06:58:56.891270Z","compaction_level":0,"original_size":0,"labels":["logit","parity","sqlite"],"dependencies":[{"issue_id":"bd-ohq","depends_on_id":"bd-1kq","type":"parent-child","created_at":"2026-02-25T06:58:56.856007Z","created_by":"themrb"}],"comments":[{"id":67,"issue_id":"bd-ohq","author":"HoangNB","text":"Goal:\nVerify mirror integrity by comparing record counts and critical fields between JSONL and SQLite outputs.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:56Z"}]}
{"id":"bd-smr","title":"Parse Codex TUI and desktop logs as diagnostic events","description":"Capture Codex runtime logs as tagged diagnostic events for troubleshooting without conflating them with chat content.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-25T06:58:30.099852Z","created_by":"themrb","updated_at":"2026-02-25T06:58:30.134771Z","compaction_level":0,"original_size":0,"labels":["adapter","codex","logit","logs"],"dependencies":[{"issue_id":"bd-smr","depends_on_id":"bd-398","type":"parent-child","created_at":"2026-02-25T06:58:30.101302Z","created_by":"themrb"}],"comments":[{"id":44,"issue_id":"bd-smr","author":"HoangNB","text":"Goal:\nCapture Codex runtime logs as tagged diagnostic events for troubleshooting without conflating them with chat content.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:30Z"}]}
{"id":"bd-soe","title":"Epic: CLI Skeleton, Command Surface, and Runtime Plumbing","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.174448Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.658971Z","compaction_level":0,"original_size":0,"labels":["cli","epic","logit"],"dependencies":[{"issue_id":"bd-soe","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.175542Z","created_by":"themrb"}],"comments":[{"id":3,"issue_id":"bd-soe","author":"HoangNB","text":"Intent:\nCreate a robust Rust CLI shell that supports command growth without command-sprawl.\n\nScope:\n- `snapshot`, `normalize`, `inspect`, `validate` command family\n- global flags and predictable error model\n- filesystem-safe runtime behavior and clear exit codes\n\nDesign principle:\nThe command interface is part of the public API and must stay stable once published.\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-ty9","title":"Write README quickstart and command usage guide","description":"Document installation assumptions, command usage patterns, and primary workflows for first-time contributors.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.482953Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.524086Z","compaction_level":0,"original_size":0,"labels":["docs","logit","readme"],"dependencies":[{"issue_id":"bd-ty9","depends_on_id":"bd-5k4","type":"parent-child","created_at":"2026-02-25T06:58:57.484418Z","created_by":"themrb"}],"comments":[{"id":78,"issue_id":"bd-ty9","author":"HoangNB","text":"Goal:\nDocument installation assumptions, command usage patterns, and primary workflows for first-time contributors.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-u6d","title":"Epic: Validation and Consistency Reports","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.512433Z","created_by":"themrb","updated_at":"2026-02-25T06:56:30.087967Z","compaction_level":0,"original_size":0,"labels":["epic","logit","validate"],"dependencies":[{"issue_id":"bd-u6d","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.513645Z","created_by":"themrb"}],"comments":[{"id":13,"issue_id":"bd-u6d","author":"HoangNB","text":"Intent:\nShip a strict but practical validation layer that certifies normalized outputs.\n\nScope:\n- schema validation\n- invariant checks (required keys, timestamp sanity)\n- machine-readable validation report and exit codes\n","created_at":"2026-02-25T06:56:30Z"}]}
{"id":"bd-ubx","title":"Write architecture and data model documentation","description":"Document internal module boundaries, adapter strategy, and canonical data model rationale.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-25T06:58:57.549088Z","created_by":"themrb","updated_at":"2026-02-25T06:58:57.591781Z","compaction_level":0,"original_size":0,"labels":["architecture","docs","logit"],"dependencies":[{"issue_id":"bd-ubx","depends_on_id":"bd-5k4","type":"parent-child","created_at":"2026-02-25T06:58:57.550257Z","created_by":"themrb"}],"comments":[{"id":79,"issue_id":"bd-ubx","author":"HoangNB","text":"Goal:\nDocument internal module boundaries, adapter strategy, and canonical data model rationale.\n\nBackground / reasoning:\nThis work item closes a quality, operability, or maintainability gap needed for production-grade local tooling.\n\nImplementation notes:\n- Keep behavior deterministic and observable.\n- Prefer explicit failure reporting over silent fallback.\n- Preserve parity with canonical event semantics.\n\nDefinition of done:\n- Output is reproducible and documented.\n- Validation and/or tests exist for core behavior.\n- Dependent tasks can proceed without hidden assumptions.\n","created_at":"2026-02-25T06:58:57Z"}]}
{"id":"bd-xdl","title":"Epic: OpenCode Adapter","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-25T06:56:29.420621Z","created_by":"themrb","updated_at":"2026-02-25T06:56:29.941936Z","compaction_level":0,"original_size":0,"labels":["adapter","epic","logit","opencode"],"dependencies":[{"issue_id":"bd-xdl","depends_on_id":"bd-1d7","type":"parent-child","created_at":"2026-02-25T06:56:29.421920Z","created_by":"themrb"}],"comments":[{"id":10,"issue_id":"bd-xdl","author":"HoangNB","text":"Intent:\nNormalize OpenCode by joining session message metadata with session part payloads.\n\nConsiderations:\n- content frequently resides in part records, not message rows\n- require reliable join keys (`messageID`, `sessionID`)\n- preserve provider/model/cost metadata where available\n","created_at":"2026-02-25T06:56:29Z"}]}
{"id":"bd-yr2","title":"Parse OpenCode runtime logs and prompt history as auxiliary sources","description":"Capture OpenCode runtime logs and prompt-history artifacts with appropriate event typing and tags.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-25T06:58:31.128436Z","created_by":"themrb","updated_at":"2026-02-25T06:58:31.164783Z","compaction_level":0,"original_size":0,"labels":["adapter","aux","logit","opencode"],"dependencies":[{"issue_id":"bd-yr2","depends_on_id":"bd-xdl","type":"parent-child","created_at":"2026-02-25T06:58:31.130537Z","created_by":"themrb"}],"comments":[{"id":62,"issue_id":"bd-yr2","author":"HoangNB","text":"Goal:\nCapture OpenCode runtime logs and prompt-history artifacts with appropriate event typing and tags.\n\nBackground / reasoning:\nThis task captures source-specific behavior that must be preserved while translating into the common event model.\nAccuracy matters more than cleverness; normalize conservatively and preserve provenance.\n\nImplementation notes:\n- Document assumptions discovered in real local files.\n- Handle malformed or partial records without crashing full runs.\n- Tag diagnostic-only sources distinctly from conversational records.\n\nDefinition of done:\n- Parsing/mapping behavior is implemented for the scoped source shapes.\n- Output aligns with canonical normalization semantics.\n- Error handling paths are explicit and testable.\n","created_at":"2026-02-25T06:58:31Z"}]}
